---
typora-copy-images-to: pictures
---

## 机器学习（吴恩达系列课程）

### 1-2什么是机器学习

在没有明确设置使的情况下，计算机具有学习能力的研究领域。  --Arthur Samuel

计算机从经验E中学习，解决某一任务T，进行某一性能度量P,通过P测定在T上的表现因经验E而提高。 	--Tom

机器学习分为监督学习和无监督学习两类。

>监督学习：正确的答案已经被给出了、教会计算机做一些事情
>
>无监督学习：不会告诉计算机正确的答案或者分类、让计算机自己学会做一些事情

### 1-3 监督学习(supervised Learning）

回归问题（Regression ）:预测连续的数组输出的问题。

分类问题(Classification):  具有离散的输出值（0 or 1）

eg：预测房价的问题是一个监督学习、回归问题。

<img src="pictures\预测房价问题.png" alt="image-20200805104150357" style="zoom:67%;" />

预测一个肿瘤是良性还是恶性是一个监督学习、分类问题(Classification)

<img src="pictures\预测肿瘤良性与否问题.png" alt="image-20200805104254669" style="zoom:67%;" />

### 1-4 无监督学习（Unsupervised Learning）

聚类问题：给定一个数据集，判断那些数据属于一个集合。

鸡尾酒问题：将不同的音频拆分开。

eg：Google等搜索引擎的页面，相似的话题会聚在一起、DNA检测、对客户分类、天文分析等都属于聚类问题。

<img src="pictures\无监督学习.png" alt="image-20200805104645344" style="zoom:50%;" />

### 2-1 模型描述

对于训练集，通过机器学习算法，训练出能很好的拟合训练集数据的假设函数h。

对于给定的输入x，就可以用h来预测想要得到的输出值y。

<img src="pictures\模型描述.png" alt="image-20200805105602338" style="zoom:80%;" />

### 2-2 代价函数(Cost function)

为了能够更快地选择出参数使得假设函数能够更好的拟合数据，定义预测值和真实输出值之间的误差为代价函数。

可以使用均方误差作为代价函数，也可以是其他的函数形式。一般来说，线性回归问题常使用均方误差，而分类问题一般不使用均方误差。

<img src="pictures\代价函数.png" alt="image-20200805110022389" style="zoom:67%;" />

### 2-3 代价函数（一）

为什么使用代价函数？

我们希望找到一条与数据拟合的直线，我们构造了假设函数h，包含两个参数。

随着所选择的参数不同，将会得到不同的直线。

代价函数就是我们要优化的目标。

<img src="pictures\代价函数1.png" alt="image-20200805110617025" style="zoom: 80%;" />

从下述的列子可以看出，当代价函数最小时，模式和数据能够最好的拟合。这时的假设函数也就是最拟合训练集的数据。

所以，代价函数最小时的参数值确实是我们想找的最优参数值。可以将代价函数作为我们的目标。

<img src="pictures\代价函数2.png" alt="image-20200805111037403" style="zoom:80%;" />

### 2-4 代价函数（二)

如下图，当改变参数的取值时，代价函数（纵轴）的值也会发生变化。当代价函数最小时，参数最优

<img src="pictures\代价函数3.png" alt="image-20200805111434022" style="zoom:67%;" />



### 2-5 梯度下降

梯度下降用来找到代价函数最小时的参数值的方法。是优化参数的一种方法。

工作原理是：从某一参数开始，通过改变参数值，不断地减小代价函数的值，最终得到局部或者全局代价函数最小值。

<img src="pictures\梯度下降.png" alt="image-20200805112154050" style="zoom:80%;" />

<img src="pictures/梯度下降1.png" alt="image-20200805112215264" style="zoom:80%;" />

从图看出：不同的初始值，最后可能会有不同的参数取值。

梯度下降的实现如下：

​								·     

<img src="pictures/梯度下降2.png" alt="image-20200805112318787" style="zoom:80%;" />

其中，必须同时更新各个参数的值。

反复该算法执行，直到函数收敛。但是得到的可能是局部最优，而不是全局最优值。

![image-20200805113819607](pictures/梯度下降3.png)

a是学习率，决定了梯度下降时的步长，也就是每一次参数改变的量。学习率太小，收敛过慢；学习率太大，函数可能会无法收敛。

<img src="pictures/梯度下降4.png" alt="image-20200805113249234" style="zoom:67%;" />

导数项是代价函数关于某一当前参数的导数，在图像上就是代价函数在此处的坡度/斜率。导数为正，参数减小；导数为负，参数增加。最终参数都是向代价函数最小的方向靠近。

<img src="pictures/梯度下降-导数.png" alt="image-20200805113454856" style="zoom: 50%;" />

### 2-7 线性回归的梯度下降

用==梯度下降==方法来求解==线性回归==的最小代价函数。

<img src="pictures/梯度下降-线性回归.png" alt="image-20200807082127295" style="zoom:80%;" />

将线性回归中的代价函数J带入梯度下降中进行求导，化简结果如下所示：

<img src="pictures/梯度下降-线性回归1.png" alt="image-20200807082531359" style="zoom: 67%;" />



并将化简后的值带入梯度下降的公式中，得到：

<img src="pictures/梯度下降-线性回归2.png" alt="image-20200807082819149" style="zoom:80%;" />

通过上式来同步的跟新两个参数。

线性回归的代价函数总是一个凸函数（如下图所示），也就是说：当用梯度下降法求解线性回归问题的时候，不会陷入求得局部最优解的误差当中去。

![image-20200807083152893](pictures/梯度下降-线性回归3.png)

像这样，梯度下降每走一步，都会遍历整个样本空间的算法，也叫做“Batch“梯度下降算法。当然，并不是所有的机器学习算法每跟新一步要遍历整个样本空间，有些算法只需要用到所有样本的一个子集。

![image-20200807083337385](pictures/batch.png)

相比于正规方程法，梯度下降法更适合于大量的样本数据时的求解。

### 3 线性代数相关知识复习

该章笔记详见GoodNote，改处不再赘述。

该章主要讲了矩阵、向量的含义、表示、基本运算和性质；矩阵的逆和转置等等。

### 41 - 4-2  多特征的线性回归

当样本的特征不止一个时，我们用小标$$i$$来表示样本$$j$$的第$$i$$个特征。

下例【预测房价】中，当输入样本的特征值含有：面积、卧室数量……

<img src="pictures/多特征线性回归.png" alt="image-20200807084210648" style="zoom:80%;" />

当特征值变多时，假设函数的形式会发生改变，要包含所有的特征参数。假设函数化简形式如下：

<img src="pictures/多特征线性回归1.png" alt="image-20200807091639296" style="zoom: 80%;" />

对应的多特征时的代价函数如下：

<img src="pictures/多特征线性回归代价函数.png" alt="image-20200807093803052" style="zoom:67%;" />

其梯度下降的函数变为下面的形式： 

<img src="pictures/多特征线性回归代价函数2.png" alt="image-20200807094137998" style="zoom:67%;" />

### 4-3 多元特征之特征缩放

feature scaling ：确保所有的特征都在一个相似的范围里面。

当两个特征值的范围相差较大时，代价函数的收敛速度会比特征值范围相近时慢很多。

<img src="pictures/多特性-特征缩放.png" alt="image-20200807094652923" style="zoom:67%;" />

一般的，我们会使得每个特征的范围在$$-1<=x_i<=1$$,但是如果范围是$$-3<=x_i<=+3$$也是ok的。

==均值归一化==Mean normalization：

使得每个特征值具有为0的平均值。特征值减去平均值再除以最大值。

<img src="pictures/多特性-特征缩放2.png" alt="image-20200807095138213" style="zoom:80%;" />

### 4-4 多元特征之学习率

通过绘制并查看J-times 曲线来观察是否随着迭代次数的增加，j的值在不断变小，依次来查看梯度下降算法是否正常工作，以及可以依靠图像来查看函数是否已经收敛；也可以通过判断每次J的值的变化是否小于阈值（$10^-3$）来判断是否收敛。

<img src="pictures/多特征-学习率.png" alt="image-20200807101344040" style="zoom:67%;" />

如果假设函数J随迭代次数的上升而上升或者时而上升时而下降，则有可能是由于学习率太小的缘故。数学证明：只要学习率足够小，则J一定会随着迭代次数的增加而下降的。

<img src="pictures/多特征-学习率2.png" alt="image-20200807101540476" style="zoom:80%;" />

学习率太大或者太小都会有问题，我们应该不断地尝试学习率的值。可以从0.001开始，0.003、0.01、0.03、1……来尝试学习率的值。这样我们会得到一个在可以正常收敛情况下的最大的学习率。也就是说在可以收敛的情况下，尽可能的使下降的步伐变大，增快收敛的速度。

<img src="pictures/多特征-学习率3.png" alt="image-20200807101819812" style="zoom:67%;" />

### 4-5 特征和多项式回归

房屋关于面积的预测假设函数如果只是用一次或者二次函数来拟合的话，会有很大的误差。可以用三次的多项式函数来预测。

当使用三次多项式函数时，可以将x当做$$x_1$$,$$x^2当做x_2$$，$x^3当做x_3$,则可以将多项式函数转化为线性函数，利用之前学过的线性回归来求解。

需要注意的是：使用该方式时，特征缩放尤为重要。

<img src="pictures/多项式回归.png" alt="image-20200807173529780" style="zoom:67%;" />

在这个例子中，取根号比二次要更为合理一点。所以，应该根据实际情况选择合适的特征。

![image-20200807174106863](pictures/多项式回归2.png)

### 4-6 正规方程

正规方程是不同于梯度下降的另外一种求解代价函数最小时的参数时的一种方法。

我们使用梯度下降的目的是为了找到代价函数的最小值，那么，如果直接用代价函数对参数求导，使得导数值为0，则可以直接得到想要的参数。

<img src="pictures/正规方程.png" alt="image-20200807182806705" style="zoom:67%;" />

例如，有四个样本，我们构造$$X$$矩阵，表示样本，其中一行表示一个样本，一列表示一个相同的特征值，向量y表示4个样本的实际的房屋价格值。

则可以使用下述的方程求解参数矩阵，证明过程略。

<img src="pictures/正规方程2.png" alt="image-20200807202314083" style="zoom:67%;" />



相比于梯度下降，特征方程法不需要选择学习率、不需要啊迭代。但是当样本数据m非常大的时候，梯度下降仍然能够很好的工作，但是对于特征方程，矩阵求逆的时间复杂度是$$O(n^3)$$。（当n<10000时，梯度下降都是ok的，但是当）

### 5 编程语言

是Octave编程语言，所以暂时跳过了。



### 6 逻辑回归之分类问题

分类问题举例：

<img src="pictures/分类问题.png" alt="image-20200807204100427" style="zoom:67%;" />

一般情况，对于二分类问题，预测值的结果是0或者1，并且0表示没有，1表示有。对于多分类问题，也可以用0、1、2、3……表示。

首先考虑使用线性函数来拟合数据集。那么当$$h_t(x)$$大于等于0.5，表示预测值为1，小于0.5，表示预测值为0。

<img src="pictures/分类问题2.png" alt="image-20200807204532411" style="zoom:67%;" />

如果依旧使用线性回归的假设函数来预测这个二分类问题，那么，当增加了最右面的那个样本之后，假设函数将会从洋红色的线变成蓝色的线，虽然，对于新增加的那个样本来说，模型对他的预测依旧是准确的，但是显然，蓝色的线对于左边的一些数据是不合理的。

通常来说，将线性回归使用到分类问题，并不是一个很好的注意。

### 6-2 逻辑回归之假设函数

对于分类问题，我们所需要的输出通常是0或者1，但是假设函数的输出通常是会大于1或者小于0的。

我们可以使用sigmoid函数（或者logistic函数），将其作用于线性回归的假设函数的输出，这样就会将原本假设函数的输出值映射到0-1区间。$$h_0(x)$$表示的是对于输入样本x使得$$ y=1$$的概率大小。

<img src="pictures/分类问题之假设函数.png" alt="image-20200807205423887" style="zoom:67%;" />

### 6-3 逻辑回归之决策边界

对于二分类问题，当假设函数的参数确定时，样本可以根据假设函数预测值大于等于0还是小于0分为两块。

这条将划分区域的线就是决策边界。他是假设函数的性质，与样本存在与否没有任何关系。

<img src="pictures/分类问题之决策边界.png" alt="image-20200807210636646" style="zoom:67%;" />

上述是当假设函数为一次时的决策边界，下面是当假设函数为高阶时的决策边界。随着假设函数中多项式的复杂多变以及参数选择的变化，我们可以得到不同的决策边界。

<img src="pictures/分类问题之决策边界2.png" alt="image-20200807212207939" style="zoom:67%;" />

### 6-4  逻辑回归之代价函数

对于下述的条件，我们有m个样本和假设函数，如何选择参数呢？

<img src="pictures/分类问题之代价函数.png" alt="image-20200807212602026" style="zoom:67%;" />

在线性回归中，我们选取的代价函数时平方差之和除以m，对于逻辑回归而言，其假设函数是非线性的，这时，如果继续采用原来的均方差做代价函数，得到的代价函数关于参数的函数是非凸的。那么，就无法使用梯度下降来求得最优值。

<img src="pictures/分类问题之代价函数2.png" alt="image-20200808102308458" style="zoom:67%;" />

因此，我们不使用均方差作为逻辑回归的代价函数。而使用下述的log函数。

<img src="pictures/逻辑回归代价函数.png" alt="image-20200808103039698" style="zoom:67%;" />

当y=1，预测值越靠近1，代价函数值越小。预测值为1时，代价函数为0；预测值为0时，代价函数值接近无穷大。

<img src="pictures/逻辑回归代价函数2.png" alt="image-20200808103834507" style="zoom:67%;" />

y=0时，预测值越靠近0，代价函数值越小；预测值越靠近1，代价函数的值越大。

### 6-5 简化代价函数和梯度下降

为了后续用梯度下降时对代价函数求导时，不用分情况讨论，将逻辑回归的代价函数简化为下述的形式：

<img src="pictures/简化的代价函数.png" alt="image-20200808104515056" style="zoom:67%;" />

有了代价函数，如何利用梯度下降求解最优参数呢？

将代价函数求导，并直接用梯度下降法求解。

<img src="pictures/逻辑回归梯度下降3.png" alt="image-20200808105231817" style="zoom:67%;" />

我们发现，逻辑回归的梯度下降式子和线性回归的式子是一模一样的，但是，这两个式子其实并不相同，因为其中的假设函数不相同。

### 6-6 高级优化

除了梯度下降，还有一些其他的方法可以求解最优的参数。相比于梯度下降，这些方法更加高效，也更加困难。



<img src="pictures/其他高级优化方法.png" alt="image-20200808110502304" style="zoom:67%;" />

### 6-7 多元分类

当分类问题中，最后需要分成的类别大于2时，这就是一个多分类问题。

如下图，是一个三元分类问题，我们可以训练三个分类器，在第一个分类器中，第一种样本为正样例，其余为负样例；第二个分类器中，第二种样本为正样例，其余为负样例。

<img src="pictures/多元分类.png" alt="image-20200808110944435" style="zoom:67%;" />

当我们有一个输入x时，将其分别输入到三个分类器中，最终，预测值最大的那个分类器就是对应的预测结果。

<img src="pictures/多元分类2.png" alt="image-20200808111136115" style="zoom:67%;" />

### 7-1 过拟合问题

如下图所示，对于给定的样本集，当多项式次数足够高，多项式足够复杂时，模型虽然能够很好的拟合数据，但是已经失去了意义。

这种情况叫做过拟合或者高方差问题。过拟合现象通常发生在样本集较少，但是样本特征较多的情况。

<img src="pictures/过拟合.png" alt="image-20200808112114052" style="zoom:67%;" />

对于分类问题，也可能存在过拟合的问题。

<img src="pictures/过拟合逻辑回归.png" alt="image-20200808112340727" style="zoom:67%;" />

当发生过拟合的问题时，如何解决呢？

一种方法是绘制假设模型曲线，直观地观察模型是否存在过拟合的现象。但是当特征值不值一个的时候，绘图会变得很困难。

可以通过人工选择一些重要的特征，舍弃一些特征来解决过拟合的问题，也可以采用一些算法来自动选择特征。

但是在舍弃一些特征的时候，也舍弃了一些信息，可能会影响模型的准确性。所以，常用的一种方法是**正则化**，对每个参数加一个惩罚因子，缩小参数的值。

<img src="pictures/解决过拟合.png" alt="image-20200808113248623" style="zoom:67%;" />

### 7-2  正则化之代价函数

